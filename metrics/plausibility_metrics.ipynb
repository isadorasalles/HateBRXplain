{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Handle common misinterpretations from double encoding\n",
    "    replacements = {\n",
    "        'Ã¡': 'á', 'Ã©': 'é', 'Ã\\xad': 'í', 'Ã³': 'ó', 'Ãº': 'ú',\n",
    "        'Ã£': 'ã', 'Ãµ': 'õ', 'Ã¢': 'â', 'Ãª': 'ê', 'Ã´': 'ô',\n",
    "        'Ã§': 'ç', 'Ã ': 'à', 'Ãš': 'Ú', 'Ã\\x81': 'Á', 'Ã‰': 'É',\n",
    "        'Ã\\x8d': 'Í', 'Ã“': 'Ó', 'Ãš': 'Ú', 'Ã“': 'Ó'\n",
    "    }\n",
    "    for wrong, right in replacements.items():\n",
    "        text = text.replace(wrong, right)\n",
    "    return text\n",
    "\n",
    "def read_files(file, model, method):\n",
    "    with open(file, 'r', encoding=\"latin1\") as f:\n",
    "        lines = f.readlines()\n",
    "        for l in lines:\n",
    "            if l.strip():  # This checks that the line is not empty or just a newline\n",
    "                # Decode and correct the encoding issues before evaluation\n",
    "                corrected_line = l.encode('latin1').decode('utf-8', errors='replace')\n",
    "                corrected_line = clean_text(corrected_line)\n",
    "                \n",
    "                # Use eval to convert string to list of tuples\n",
    "                data = list(eval(corrected_line))\n",
    "                \n",
    "                data.sort(key=lambda tup: tup[1], reverse=True)\n",
    "                pred[model][method].append([i[0].replace(' ', '') for i in data if i[1] > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred = {}\n",
    "\n",
    "for model in ['bertimbau', 'distilbertimbau', 'ptt5', 'mbert']:\n",
    "    pred[model] = {}\n",
    "    for method in ['lime', 'shap']:\n",
    "        pred[model][method] = []\n",
    "        paths = []\n",
    "        for name in glob.glob('../results/'+model+'_'+method+'*'):\n",
    "            paths.append(name)\n",
    "\n",
    "        for filename in sorted(paths):    \n",
    "            read_files(filename, model, method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataa/hatebr_and_rationales.csv', \\\n",
    "                index_col=0, \\\n",
    "                converters={\"rationales_offensive_1_normalized\": \\\n",
    "                lambda x: x.strip('[]').replace(\"'\", \"\").split(\", \"),\n",
    "                \"rationales_offensive_2_normalized\": \\\n",
    "                lambda x: x.strip('[]').replace(\"'\", \"\").split(\", \")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split to train and val\n",
    "TRAIN_SIZE = 0.8\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "x_train, x_test_val, y_train, y_test_val = train_test_split(df['normalized_text'], df['label final'], test_size=TEST_SIZE + VAL_SIZE, random_state=0)\n",
    "x_test, x_val, y_test, y_val = train_test_split(x_test_val, y_test_val, test_size=VAL_SIZE/(TEST_SIZE + VAL_SIZE), random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "instances = np.where(y_train == 1)[0][:350]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_match_score(a, b, threshold=0.5):\n",
    "        \n",
    "    intersection= list(set(a) & set(b))\n",
    "    union = list(set().union(a, b))\n",
    "    iou = len(intersection)/len(union)\n",
    "    return iou >= threshold\n",
    "    \n",
    "    \n",
    "def iou_f1_score(annot1, annot2, pred, model, method):\n",
    "    \n",
    "    threshold_tps = 0\n",
    "    count=0\n",
    "    count2 = 0\n",
    "    for e, (r1, r2, p) in enumerate(zip(annot1, annot2, pred[model][method])):\n",
    "        \n",
    "        if e in to_remove_index:\n",
    "            continue\n",
    "        \n",
    "        a = ' '.join(r1).split(' ')\n",
    "        b = ' '.join(r2).split(' ')\n",
    "        \n",
    "        if partial_match_score(a, p) or partial_match_score(b, p):\n",
    "            if partial_match_score(a, p):\n",
    "                count+=1\n",
    "            if partial_match_score(b, p):\n",
    "                count2+=1\n",
    "            threshold_tps += 1\n",
    "        \n",
    "    precision = threshold_tps/350\n",
    "    recall = threshold_tps/350\n",
    "    f1_score = (2*((recall*precision)/(recall+precision)))\n",
    "    \n",
    "    return f1_score\n",
    "\n",
    "\n",
    "def compute_f1(a, b):\n",
    "    p = len(list(set(a) & set(b)))/len(a) if len(a) > 0 else 0\n",
    "    r = len(list(set(a) & set(b)))/len(b) if len(b) > 0 else 0\n",
    "    f1 = 2*((r*p)/(r+p)) if r+p > 0 else 0\n",
    "    return p, r, f1\n",
    "\n",
    "def f1_score_token_level(annot1, annot2, pred, model, method):\n",
    "    f1_score = []\n",
    "    recs = []\n",
    "    precs = []\n",
    "    count = 0\n",
    "    for e, (r1, r2, p) in enumerate(zip(annot1, annot2, pred[model][method])):\n",
    "        \n",
    "        if e in to_remove_index:\n",
    "            continue\n",
    "        \n",
    "        a = ' '.join(r1).split(' ')\n",
    "        b = ' '.join(r2).split(' ')\n",
    "        \n",
    "        \n",
    "        prec1, rec1, f11 = compute_f1(p, a)\n",
    "        prec2, rec2, f12 = compute_f1(p, b)\n",
    "        \n",
    "        if f12 > f11:\n",
    "            count += 1\n",
    "            f1_score.append(f12)\n",
    "            recs.append(rec2)\n",
    "            precs.append(prec2)\n",
    "        else:\n",
    "            f1_score.append(f11)\n",
    "            recs.append(rec1)\n",
    "            precs.append(prec1)\n",
    "\n",
    "\n",
    "    return np.mean(f1_score), np.mean(precs), np.mean(recs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_instances = x_train.iloc[instances].to_frame().join(df.set_index('normalized_text'), on='normalized_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['Method'] = []\n",
    "results['IOU_F1'] = []\n",
    "results['Token-level Precision'] = []\n",
    "results['Token-level Recall'] = []\n",
    "results['Token-level F1'] = []\n",
    "\n",
    "\n",
    "for model in ['mbert', 'bertimbau', 'distilbertimbau', 'ptt5']:\n",
    "    for method in ['lime', 'shap']:\n",
    "\n",
    "        results['Method'].append(model+'_'+method)\n",
    "        results['IOU_F1'].append(iou_f1_score(df_instances['rationales_offensive_1_normalized'], \\\n",
    "            df_instances['rationales_offensive_2_normalized'], \n",
    "           pred, model, method))\n",
    "\n",
    "        f1, prec, rec = f1_score_token_level(df_instances['rationales_offensive_1_normalized'], \\\n",
    "            df_instances['rationales_offensive_2_normalized'], \n",
    "           pred, model, method)\n",
    "        results['Token-level Precision'].append(prec)\n",
    "        results['Token-level Recall'].append(rec)\n",
    "        results['Token-level F1'].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>IOU_F1</th>\n",
       "      <th>Token-level Precision</th>\n",
       "      <th>Token-level Recall</th>\n",
       "      <th>Token-level F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mbert_lime</td>\n",
       "      <td>0.582857</td>\n",
       "      <td>0.745764</td>\n",
       "      <td>0.693730</td>\n",
       "      <td>0.670273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mbert_shap</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.714090</td>\n",
       "      <td>0.752051</td>\n",
       "      <td>0.689625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bertimbau_lime</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>0.755398</td>\n",
       "      <td>0.684847</td>\n",
       "      <td>0.669823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bertimbau_shap</td>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.748548</td>\n",
       "      <td>0.709884</td>\n",
       "      <td>0.682974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distilbertimbau_lime</td>\n",
       "      <td>0.645714</td>\n",
       "      <td>0.761399</td>\n",
       "      <td>0.727633</td>\n",
       "      <td>0.700331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>distilbertimbau_shap</td>\n",
       "      <td>0.617143</td>\n",
       "      <td>0.753886</td>\n",
       "      <td>0.686166</td>\n",
       "      <td>0.671761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ptt5_lime</td>\n",
       "      <td>0.605714</td>\n",
       "      <td>0.748172</td>\n",
       "      <td>0.697772</td>\n",
       "      <td>0.677533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ptt5_shap</td>\n",
       "      <td>0.740000</td>\n",
       "      <td>0.717420</td>\n",
       "      <td>0.837881</td>\n",
       "      <td>0.736063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Method    IOU_F1  Token-level Precision  Token-level Recall  \\\n",
       "0            mbert_lime  0.582857               0.745764            0.693730   \n",
       "1            mbert_shap  0.660000               0.714090            0.752051   \n",
       "2        bertimbau_lime  0.585714               0.755398            0.684847   \n",
       "3        bertimbau_shap  0.657143               0.748548            0.709884   \n",
       "4  distilbertimbau_lime  0.645714               0.761399            0.727633   \n",
       "5  distilbertimbau_shap  0.617143               0.753886            0.686166   \n",
       "6             ptt5_lime  0.605714               0.748172            0.697772   \n",
       "7             ptt5_shap  0.740000               0.717420            0.837881   \n",
       "\n",
       "   Token-level F1  \n",
       "0        0.670273  \n",
       "1        0.689625  \n",
       "2        0.669823  \n",
       "3        0.682974  \n",
       "4        0.700331  \n",
       "5        0.671761  \n",
       "6        0.677533  \n",
       "7        0.736063  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mbert lime 5.311428571428571\n",
      "mbert shap 8.957142857142857\n",
      "bertimbau lime 5.317142857142857\n",
      "bertimbau shap 7.64\n",
      "distilbertimbau lime 5.5685714285714285\n",
      "distilbertimbau shap 7.28\n",
      "ptt5 lime 5.408571428571428\n",
      "ptt5 shap 10.042857142857143\n"
     ]
    }
   ],
   "source": [
    "# average tokens generated per model\n",
    "for model in ['mbert', 'bertimbau', 'distilbertimbau', 'ptt5']:\n",
    "    for method in ['lime', 'shap']:\n",
    "        avg_tokens = 0\n",
    "        for r in pred[model][method]:\n",
    "            avg_tokens += len(r)\n",
    "        print(model, method, avg_tokens/350)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
